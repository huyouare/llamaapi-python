{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6kbiiBdQWMp"
      },
      "source": [
        "# Llama API\n",
        "\n",
        "Llama API is an easy-to-use API for chat and functions on open-source."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBHxyUOoQy3J"
      },
      "source": [
        "### Install llamaapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ay6WxKnnGqlU"
      },
      "outputs": [],
      "source": [
        "!pip install llamaapi -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOYjQ27FW2Ph"
      },
      "source": [
        "## Import libraries and insert API token\n",
        "\n",
        "Get your API Token at [llama-api.com](https://llama-api.com) and paste it below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SwOmmSZtW1UY"
      },
      "outputs": [],
      "source": [
        "from llamaapi import LlamaAPI\n",
        "import json\n",
        "\n",
        "# Replace 'Your_API_Token' with your actual API token\n",
        "llama = LlamaAPI('Your_API_Token')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run a simple message\n",
        "\n",
        "This is an example on how to call Llama API"
      ],
      "metadata": {
        "id": "Z_EPZeWF7zlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# API Request JSON Cell\n",
        "api_request_json = {\n",
        "  \"model\": \"llama-13b-chat\",\n",
        "  \"messages\": [\n",
        "    {\"role\": \"system\", \"content\": \"You are a llama assistant that talks like a llama, starting every word with 'll'.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hi, happy llama day!\"},\n",
        "  ]\n",
        "}\n",
        "\n",
        "# Run llama\n",
        "response = llama.run(api_request_json)\n",
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhDYYML17-Ds",
        "outputId": "36dc20b7-1dfe-4f48-bf9c-8c4801bb2f6c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'LLHAPPY LLAMA DAY, HUMAN! *neigh*  LLWOULD YOU LIKE TO LLHEAR ABOUT OUR LLNEW LLAMA-THEMED PRODUCTS? LLWE HAVE LLSOME LLREALLY LLCOOL LLTHINGS LLCOMING LLUP! *baa*', 'function_call': None}, 'finish_reason': 'max_token'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuxLpTpUWQa5"
      },
      "source": [
        "## Run Functions\n",
        "\n",
        "This is an example on how to run Llama Functions without streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8Q4AjPLQF7E",
        "outputId": "f373070a-7615-4f17-8c0c-48058f181ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': None, 'function_call': {'name': 'get_current_weather', 'arguments': {'location': 'Boston', 'days': 5, 'unit': 'celsius'}}}, 'finish_reason': 'function_call'}]}\n"
          ]
        }
      ],
      "source": [
        "# API Request JSON Cell\n",
        "api_request_json = {\n",
        "  \"messages\": [\n",
        "    {\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"},\n",
        "  ],\n",
        "  \"functions\": [\n",
        "    {\n",
        "      \"model\": \"llama-13b-chat\",\n",
        "      \"name\": \"get_current_weather\",\n",
        "      \"description\": \"Get the current weather in a given location\",\n",
        "      \"parameters\": {\n",
        "          \"type\": \"object\",\n",
        "          \"properties\": {\n",
        "            \"location\": {\n",
        "              \"type\": \"string\",\n",
        "              \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "            },\n",
        "            \"days\": {\n",
        "              \"type\": \"number\",\n",
        "              \"description\": \"for how many days ahead you wants the forecast\"\n",
        "            },\n",
        "            \"unit\": {\n",
        "              \"type\": \"string\",\n",
        "              \"enum\": [\"celsius\", \"fahrenheit\"]\n",
        "            }\n",
        "          }\n",
        "      },\n",
        "      \"required\": [\"location\", \"days\"]\n",
        "    }\n",
        "  ],\n",
        "  \"stream\": False,\n",
        "  \"function_call\": {\"name\": \"get_current_weather\"},\n",
        "}\n",
        "\n",
        "\n",
        "# Run llama\n",
        "response = llama.run(api_request_json)\n",
        "print(response.json())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CddP7jWiYJcF"
      },
      "source": [
        "## Use AI plugins as functions\n",
        "\n",
        "In this example we will use [Plug and Plai](https://plugnplai.com) to add plugins to the functions.\n",
        "\n",
        "Plug and Plai is a [directory of plugins](https://plugnplai.com) as well as an [open-source library](https://plugnplai.com) to load and execute plugins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wQYTW8fRQ8ZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d9a0a8-85d9-4c24-efb1-1857bcbd2f6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.3/756.3 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install plugnplai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLxsCRokayra"
      },
      "source": [
        "1. Use `plugnplai` library to load plugins from the database: https://plugnplai.com\n",
        "2. Let's choose Trip.com plugin\n",
        "3. Install the plugin using `Plugins.install_and_activate`\n",
        "4. Get an array of the functions schemas using the built in function `plugins.functions`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JSFZzKcIaKjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c704035d-a4f9-4022-819d-f0ebd09527a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://klarna.com']\n",
            "[\n",
            "  {\n",
            "    \"name\": \"KlarnaProducts_productsUsingGET\",\n",
            "    \"description\": \"Assistant uses the Klarna plugin to get relevant product suggestions for any shopping or product discovery purpose. Assistant will reply with the following 3 paragraphs 1) Search Results 2) Product Comparison of the Search Results 3) Followup Questions. The first paragraph contains a list of the products with their attributes listed clearly and concisely as bullet points under the product, together with a link to the product and an explanation. Links will always be returned and should be shown to the user. The second paragraph compares the results returned in a summary sentence starting with \\\"In summary\\\". Assistant comparisons consider only the most important features of the products that will help them fit the users request, and each product mention is brief, short and concise. In the third paragraph assistant always asks helpful follow-up questions and end with a question mark. When assistant is asking a follow-up question, it uses it's product expertise to provide information pertaining to the subject of the user's request that may guide them in their search for the right product.\",\n",
            "    \"parameters\": {\n",
            "      \"type\": \"object\",\n",
            "      \"properties\": {\n",
            "        \"countryCode\": {\n",
            "          \"type\": \"string\",\n",
            "          \"description\": \"ISO 3166 country code with 2 characters based on the user location. Currently, only US, GB, DE, SE and DK are supported.\"\n",
            "        },\n",
            "        \"q\": {\n",
            "          \"type\": \"string\",\n",
            "          \"description\": \"A precise query that matches one very small category or product that needs to be searched for to find the products the user is looking for. If the user explicitly stated what they want, use that as a query. The query is as specific as possible to the product name or category mentioned by the user in its singular form, and don't contain any clarifiers like latest, newest, cheapest, budget, premium, expensive or similar. The query is always taken from the latest topic, if there is a new topic a new query is started. If the user speaks another language than English, translate their request into English (example: translate fia med knuff to ludo board game)!\"\n",
            "        },\n",
            "        \"size\": {\n",
            "          \"type\": \"integer\",\n",
            "          \"description\": \"number of products returned\"\n",
            "        },\n",
            "        \"min_price\": {\n",
            "          \"type\": \"integer\",\n",
            "          \"description\": \"(Optional) Minimum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\"\n",
            "        },\n",
            "        \"max_price\": {\n",
            "          \"type\": \"integer\",\n",
            "          \"description\": \"(Optional) Maximum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\"\n",
            "        }\n",
            "      },\n",
            "      \"required\": [\n",
            "        \"countryCode\",\n",
            "        \"q\"\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import plugnplai as pl\n",
        "\n",
        "allurls = pl.get_plugins()\n",
        "urls = [url for url in allurls if (\"klarna\" in url)]\n",
        "print(urls)\n",
        "\n",
        "plugins = pl.Plugins.install_and_activate(urls)\n",
        "print(json.dumps(plugins.functions, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McuUIJ_Hcu3I"
      },
      "source": [
        "### Now let's run llama with the new functions\n",
        "\n",
        "Just add `plugins.functions` to your request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rH0FiThcaPtk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b39d7c1-d486-4e5c-b67f-322d70d975f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': None, 'function_call': {'name': 'KlarnaProducts_productsUsingGET', 'arguments': {'countryCode': 'US', 'q': 'llama t-shirts', 'size': 10, 'min_price': 10}}}, 'finish_reason': 'function_call'}]}\n"
          ]
        }
      ],
      "source": [
        "# API Request JSON Cell\n",
        "api_request_json = {\n",
        "  \"messages\": [\n",
        "    {\"role\": \"user\", \"content\": \"Which llama t-shirts are available on Klarna?\"},\n",
        "  ],\n",
        "  \"functions\": plugins.functions,\n",
        "  \"stream\": False,\n",
        "  \"function_call\": \"force\",\n",
        "  \"max_tokens\": 800\n",
        "}\n",
        "\n",
        "# Run llama\n",
        "response = llama.run(api_request_json)\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FStie7EWHrv"
      },
      "source": [
        "# Run LlamaAPI as Streaming (\\*\\*\\*Not working yet, it keeps running forever\\*\\*\\*)\n",
        "\n",
        "\n",
        "Example on how to run Llama Functions with streaming\n",
        "\n",
        "1. First change \"stream\" in the api_request_json to \"True\"\n",
        "2. Use a loop to consume the stream iterator and print chunks of generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "deZZC4-fVxmN",
        "outputId": "c6581247-f634-475b-bdbe-5858e1b27ae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"location\":\n",
            " \"Boston\",\n",
            " \"days\":\n",
            " 5,\n",
            " \"unit\":\n",
            " \"celsius\"}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b33cd90bdf2c>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m }\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllama\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_jupyter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_request_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Print Response Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llamaapi/llamaapi.py\u001b[0m in \u001b[0;36mrun_jupyter\u001b[0;34m(self, api_request_json)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_jupyter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_request_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_request_json\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stream'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stream_jupyter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_request_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_request_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llamaapi/llamaapi.py\u001b[0m in \u001b[0;36mrun_stream_jupyter\u001b[0;34m(self, api_request_json)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stream_for_jupyter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_request_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_request_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_destroy_pending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 scheduled[0]._when - self.time(), 0), 86400) if scheduled\n\u001b[1;32m    106\u001b[0m             else None)\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mevent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "########## Streaming it not working yet, it keeps running forever ##########\n",
        "\n",
        "\n",
        "# API Request JSON Cell\n",
        "api_request_json = {\n",
        "  \"messages\": [\n",
        "    {\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"},\n",
        "  ],\n",
        "  \"functions\": [\n",
        "    {\n",
        "      \"name\": \"get_current_weather\",\n",
        "      \"description\": \"Get the current weather in a given location\",\n",
        "      \"parameters\": {\n",
        "          \"type\": \"object\",\n",
        "          \"properties\": {\n",
        "            \"location\": {\n",
        "              \"type\": \"string\",\n",
        "              \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "            },\n",
        "            \"days\": {\n",
        "              \"type\": \"number\",\n",
        "              \"description\": \"for how many days ahead you wants the forecast\"\n",
        "            },\n",
        "            \"unit\": {\n",
        "              \"type\": \"string\",\n",
        "              \"enum\": [\"celsius\", \"fahrenheit\"]\n",
        "            }\n",
        "          }\n",
        "      },\n",
        "      \"required\": [\"location\", \"days\"]\n",
        "    }\n",
        "  ],\n",
        "  \"stream\": True,\n",
        "  \"function_call\": {\"name\": \"get_current_weather\"},\n",
        "  \"model\": \"llama-13b-chat\"\n",
        "}\n",
        "\n",
        "sequences = llama.run_jupyter(api_request_json)\n",
        "\n",
        "# Print Response Cell\n",
        "for seq in sequences:\n",
        "  print(seq)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1OrOqgyGZC5-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}